# Prueba T√©cnica - Refactor de Pipeline ETL (ANI)

Este repositorio contiene la soluci√≥n a una prueba t√©cnica que consiste en refactorizar un script monol√≠tico de Python (dise√±ado para AWS Lambda) a un pipeline ETL modular (Extracci√≥n, Validaci√≥n, Escritura) orquestado con Apache Airflow.

El pipeline extrae datos de normativas de la ANI, los valida contra un set de reglas configurables y los carga en una base de datos PostgreSQL, asegurando la idempotencia (no se insertan duplicados).

## üöÄ Tecnolog√≠as Utilizadas

* **Orquestaci√≥n:** Apache Airflow
* **Contenedores:** Docker y Docker Compose
* **Base de Datos:** PostgreSQL 15
* **Core:** Python 3.9
* **Librer√≠as:** Pandas, BeautifulSoup (Scraping), Psycopg2 (DB)

## üìÅ Estructura del Repositorio

-   `/configs/validation_rules.json`: Archivo JSON con las reglas de validaci√≥n (regex, tipo, etc.).
-   `/dags/dags_etl.py`: Definici√≥n del DAG principal de Airflow (`dag_etl_ani`).
-   `/src/extraction.py`: M√≥dulo de extracci√≥n (scraping).
-   `/src/validation.py`: M√≥dulo de validaci√≥n de datos.
-   `/src/write.py`: M√≥dulo de escritura (persistencia) que contiene la l√≥gica de idempotencia.
-   `DDL.sql`: Script DDL para crear las tablas `regulations` y `regulations_component`.
-   `docker-compose.yml`: Define los servicios de Airflow (webserver, scheduler) y Postgres.
-   `Dockerfile`: Define la imagen de Airflow con las dependencias de Python.

---

## üèÉ‚Äç‚ôÇÔ∏è C√≥mo Ejecutar el Proyecto

### 1. Levantar el Entorno

Inicia todos los servicios de Airflow y la base de datos de Postgres.

```bash
docker-compose up -d --build

### 2. Crear el Esquema de la Base de Datos

Espera un minuto a que el contenedor de Postgres inicie. Luego, ejecuta el siguiente comando en tu terminal para crear las tablas (aseg√∫rate de que DDL.sql est√© en tu carpeta).

# (En Windows/PowerShell)
cat DDL_corregido.sql | docker exec -i dapper_technical_test-postgres-1 psql -U airflow -d airflow

###3. Acceder a Airflow
Abre tu navegador y ve a: URL: http://localhost:8080 Usuario: admin Clave: admin

###4. Ejecutar el Pipeline
En la interfaz de Airflow, busca el DAG llamado dag_etl_ani.

Act√≠valo (con el interruptor a la izquierda).

Haz clic en el nombre del DAG y presiona el bot√≥n "Play" (‚ñ∂Ô∏è) en la esquina superior derecha para ejecutarlo.

###5. Verificar la Idempotencia
Primera Ejecuci√≥n: Revisa los logs de la tarea write_task. Deber√≠as ver un mensaje como New inserted: 29.

Segunda Ejecuci√≥n: Ejecuta el DAG una segunda vez. Revisa los logs de write_task de esta nueva ejecuci√≥n. Deber√≠as ver New inserted: 0 y No new records found.... Esto confirma que la l√≥gica de idempotencia funciona.